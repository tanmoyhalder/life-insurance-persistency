{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -V"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same code present in `persistency_base_model - modified data3.ipynb`. I have just copied the notebook and renamed it. In this notebook, I am using the new data `master_data - modified3.csv` which I have renamed to `master_data_final2.csv` in this folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from feature_engine import encoding as ce\n",
    "from feature_engine import imputation as mdi\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import mysql.connector\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the database connection parameters\n",
    "db_name = \"PersistencyDB\"\n",
    "user_name = \"root\"\n",
    "password = \"persistency_dna\"\n",
    "host_name = \"127.0.0.1\"\n",
    "port_number = 3306"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_uri = f\"mysql://{user_name}:{password}@{host_name}:{port_number}/{db_name}\"\n",
    "mlflow.set_tracking_uri(tracking_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing mlflow and setting tracking uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"persistency-prediction-experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILEPATH = 'data'\n",
    "INPUT_FILENAME = 'master_data_final2.csv'\n",
    "\n",
    "INDEX = 'policy_number'\n",
    "DATE_COLS = ['proposal_received_date', 'policy_issue_date', 'agent_dob', 'agent_doj']\n",
    "NA_VALUES = ['', 'NA', 'N/A', 'NULL', 'null', '?', '*', '#N/A', '#VALUE!']\n",
    "DTYPE_DICT = {'zipcode': 'str', 'agent_code': 'str'} ## These columns should be string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(INPUT_FILEPATH, INPUT_FILENAME):\n",
    "    input_df = pd.read_csv(os.path.join(INPUT_FILEPATH, INPUT_FILENAME),\n",
    "                      index_col = INDEX,\n",
    "                      na_values = NA_VALUES,\n",
    "                      parse_dates = DATE_COLS,\n",
    "                      dayfirst = True,\n",
    "                      dtype = DTYPE_DICT)\n",
    "                    \n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = load_data(INPUT_FILEPATH, INPUT_FILENAME)\n",
    "input_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df['lapse'].value_counts()/len(input_df)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating feature: time_to_issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_to_issue(df) -> pd.DataFrame:\n",
    "    df['time_to_issue'] = (df['policy_issue_date'] - df['proposal_received_date']).dt.days\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = create_time_to_issue(input_df)\n",
    "input_df['time_to_issue'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating feature: prem_to_income_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prem_to_income_ratio(df) -> pd.DataFrame:\n",
    "    df['prem_to_income_ratio'] = np.where(df['income'] == 0, 0, (df['annual_premium']/df['income']))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = create_prem_to_income_ratio(input_df)\n",
    "input_df['prem_to_income_ratio'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "The objective of this exercise is to create a demoable solution (maybe not the best possible one, given the augmented data). Hence, we are not going to deep dive into EDA and hypothesis testing. Instead, we will focus on building the ML product using different technologies. \n",
    "\n",
    "## --------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS_TO_REM = ['proposal_received_date','policy_issue_date', 'zipcode', 'county', 'state', 'agent_code', 'agent_dob', 'agent_doj']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `proposal_received_date`: Derived `time_to_issue` from this column\n",
    "- `policy_issue_date`: Derived `time_to_issue` from this column\n",
    "- `zipcode`: Too many values, high cardinality\n",
    "- `county`: Too many values, high cardinality\n",
    "- `state`: Too many values, high cardinality\n",
    "- `agent_code`: Id column\n",
    "- `agent_dob`: Derived `agent_age` from this column\n",
    "- `agent_doj`: Derived `agent_tenure_days` from this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df, COLS_TO_REM) -> pd.DataFrame:\n",
    "    df = df.drop(COLS_TO_REM, axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = filter_df(input_df, COLS_TO_REM)\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Value Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have only one column where missing value is present. `agent_persistency`. Impute missing value with median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_persistency_missing_perc = round(temp_df['agent_persistency'].isnull().mean()*100,2)\n",
    "\n",
    "print(f'Total missing percentage of column agent_persistency is: {agent_persistency_missing_perc}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISSING_COL = ['agent_persistency']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONE_HOT_COLS = ['owner_gender', 'marital_status', 'smoker', 'medical', 'education', 'occupation', 'payment_freq',  \n",
    "                'agent_status', 'agent_education']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalisation\n",
    "\n",
    "We will normalise the columns using `StandardScaler` because we have values at different scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['owner_age', 'owner_gender', 'marital_status', 'num_nominee', 'smoker',\n",
    "       'medical', 'education', 'occupation', 'experience', 'income',\n",
    "       'negative_zipcode', 'family_member', 'existing_num_policy',\n",
    "       'has_critical_health_history', 'policy_term', 'payment_freq',\n",
    "       'annual_premium', 'sum_insured', 'agent_status', 'agent_education',\n",
    "       'agent_age', 'agent_tenure_days', 'agent_persistency',\n",
    "       'last_6_month_submissions', 'average_premium', 'is_reinstated',\n",
    "       'prev_persistency', 'num_complaints', 'target_completion_perc',\n",
    "       'has_contacted_in_last_6_months', 'credit_score',\n",
    "       'time_to_issue', 'prem_to_income_ratio']\n",
    "\n",
    "TARGET = 'lapse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(temp_df[FEATURES],\n",
    "                                                    temp_df[TARGET],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state = 786, \n",
    "                                                    shuffle = True,\n",
    "                                                    stratify = temp_df[TARGET])\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating data transformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_pipe = Pipeline([\n",
    "    \n",
    "    ('imputer_num', mdi.MeanMedianImputer(imputation_method = 'median', variables = MISSING_COL )), \n",
    "    \n",
    "    ('onehot_encoder', ce.OneHotEncoder(top_categories=None,\n",
    "                                        variables= ONE_HOT_COLS,\n",
    "                                        drop_last=True)),\n",
    "    \n",
    "    ('normalisation', StandardScaler())\n",
    "    \n",
    "    # ('clf', LogisticRegression(penalty,random_state = 786))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trf = model_input_pipe.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = xgb.DMatrix(X_train_trf, label = y_train)\n",
    "X_test_trf = model_input_pipe.transform(X_test)\n",
    "# valid = xgb.DMatrix(X_test_trf, label = y_test) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running xgboost with hyperopt and tracking using mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag(\"developer\", \"tanmoy\")\n",
    "        mlflow.set_tag(\"model\", \"xgboost-sklearn hyperparam\")\n",
    "        mlflow.set_tag(\"type\", \"experiment\")\n",
    "\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        booster = xgb.XGBClassifier(**params)\n",
    "\n",
    "        xgboost_model = booster.fit(X_train_trf,y_train)\n",
    "\n",
    "        y_pred = xgboost_model.predict(X_test_trf)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "        roc_auc = roc_auc_score(y_test, y_pred)\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "\n",
    "        return {\"loss\": -recall, 'status': STATUS_OK}                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space =  {\n",
    "    'max_depth' : scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "    'learning_rate' : hp.loguniform('learning_rate', -3, 0),\n",
    "    'min_child_weight' : hp.loguniform('min_child_weight', -1, 3),\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "    'objective' : 'binary:logistic',\n",
    "    'seed' : 786\n",
    "}\n",
    "\n",
    "best_result = fmin(\n",
    "\n",
    "    fn = objective,\n",
    "    space = search_space,\n",
    "    algo = tpe.suggest,\n",
    "    max_evals = 30,\n",
    "    trials = Trials()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalising model \n",
    "- Running with best model params (best model chosen in terms of highest recall)\n",
    "- Autologging along with customised metrics logging\n",
    "- storing prerocessor `model_input_pipe` as an artifact\n",
    "- storing model as an artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "MLFLOW_TRACKING_URI = tracking_uri\n",
    "client = MlflowClient(tracking_uri= MLFLOW_TRACKING_URI)\n",
    "\n",
    "runs = client.search_runs(\n",
    "    experiment_ids= '1',\n",
    "    filter_string= \"tags.model = 'xgboost-sklearn hyperparam'\",\n",
    "    run_view_type= ViewType.ACTIVE_ONLY,\n",
    "    max_results= 1,\n",
    "    order_by= [\"metrics.recall DESC\"]\n",
    ")\n",
    "\n",
    "for run in runs:\n",
    "    print(f\"run id: {run.info.run_id}, recall: {run.data.metrics['recall']:.4f}\")\n",
    "\n",
    "run.info.run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS = run.data.params\n",
    "RUN_ID = run.info.run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"my-gcs-bucket-name\"\n",
    "artifact_path = \"my-artifact-path\"\n",
    "mlflow.set_experiment(\"my-experiment-name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = BEST_PARAMS\n",
    "\n",
    "with mlflow.start_run(artifact_location=f\"gcs://{bucket_name}/{artifact_path}\"):\n",
    "    mlflow.xgboost.autolog()\n",
    "    mlflow.set_tag(\"developer\", \"tanmoy\")\n",
    "    mlflow.set_tag(\"model\", \"xgboost-sklearn\")\n",
    "    mlflow.set_tag(\"type\", \"xgboost-sklearn final\")\n",
    "\n",
    "    booster = xgb.XGBClassifier(**best_params)\n",
    "    xgboost_model = booster.fit(X_train_trf,y_train)\n",
    "\n",
    "    y_pred = xgboost_model.predict(X_test_trf)\n",
    "            \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "\n",
    "    with open(\"models/preprocessor.b\", \"wb\") as f_out:\n",
    "        pickle.dump(model_input_pipe, f_out)\n",
    "\n",
    "    mlflow.log_artifact(\"models/preprocessor.b\", artifact_path = \"preprocessor\")\n",
    "    mlflow.xgboost.log_model(xgboost_model, artifact_path= \"model_mlflow\")\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Name: `amazing-hound-63`\n",
    "#### Run Id: `b878a2ba0b834edea0a44cf6935f4dc0`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model locally and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = client.search_runs(\n",
    "    experiment_ids= '1',\n",
    "    filter_string= \"tags.type = 'xgboost-sklearn final'\",\n",
    "    run_view_type= ViewType.ACTIVE_ONLY,\n",
    "    max_results= 1,\n",
    "    order_by= [\"metrics.recall DESC\"]\n",
    ")\n",
    "\n",
    "for run in runs:\n",
    "    print(f\"run id: {run.info.run_id}, recall: {run.data.metrics['recall']:.4f}\")\n",
    "\n",
    "RUN_ID = run.info.run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the uri\n",
    "logged_model = 'mlruns/1/' + RUN_ID + '/artifacts/model_mlflow/'\n",
    "\n",
    "# Load model as a PyFuncModel\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "print(loaded_model)\n",
    "\n",
    "# load as a xgboost model\n",
    "xgboost_model = mlflow.xgboost.load_model(logged_model)\n",
    "print(xgboost_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on a Pandas DataFrame.\n",
    "predictions = xgboost_model.predict_proba(X_test_trf)\n",
    "\n",
    "predicted_proba = []\n",
    "for i in range(0, len(predictions)):\n",
    "    predicted_proba.append(predictions[i][1])\n",
    "\n",
    "predicted_proba"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the distribution of `1` and `0` in the predicted vs actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df = pd.DataFrame({'y_test': y_test, 'predicted_proba': predicted_proba})\n",
    "comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data= comp_df, x = \"predicted_proba\", hue=\"y_test\", kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final_features = model_input_pipe.get_feature_names_out(input_features= FEATURES)\n",
    "\n",
    "out_feature_list = []\n",
    "for f in range(0, len(model_final_features)):\n",
    "    feat = 'f' + str(f)\n",
    "    out_feature_list.append(feat)\n",
    "\n",
    "out_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame(xgboost_model.feature_importances_, index = out_feature_list, columns= ['importantce'])\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance():\n",
    "\n",
    "    model_final_features = model_input_pipe.get_feature_names_out(input_features= FEATURES)\n",
    "\n",
    "    out_feature_list = []\n",
    "    for f in range(0, len(model_final_features)):\n",
    "        feat = 'f' + str(f)\n",
    "        out_feature_list.append(feat)\n",
    "\n",
    "    feat_df = pd.DataFrame(data = model_final_features, index = out_feature_list, columns= ['feature_names'])\n",
    "\n",
    "    importance_df = pd.DataFrame(xgboost_model.feature_importances_, index = out_feature_list, columns= ['importance'])\n",
    "    importance_df\n",
    "\n",
    "    plot_df = feat_df.merge(importance_df, how = 'inner',left_index = True, right_index= True)\n",
    "\n",
    "    sns.set(rc={\"figure.figsize\":(20, 15)})\n",
    "    # sns.barplot(data = plot_df.sort_values(by = 'score', ascending= False), y = \"feature_names\", x = \"score\", orient = 'h')\n",
    "    # plt.show()\n",
    "\n",
    "    barplot = sns.barplot(data = plot_df.sort_values(by = 'importance', ascending= False), y = \"feature_names\", x = \"importance\", orient = 'h')\n",
    "    fig = barplot.get_figure()\n",
    "    fig.savefig('xgboost-sklearn_feature_imporance.png')\n",
    "    # return plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyarrow as pa\n",
    "# import pyarrow.parquet as pq\n",
    "# import pickle\n",
    "\n",
    "# with open(\"models/X_train_trf.b\", \"wb\") as f_out:\n",
    "#             pickle.dump(X_train_trf, f_out)\n",
    "\n",
    "# feat_df_table = pa.Table.from_pandas(feat_df)\n",
    "# pq.write_table(feat_df_table, 'models/feat_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final_features = model_input_pipe.get_feature_names_out(input_features= FEATURES)\n",
    "\n",
    "out_feature_list = []\n",
    "for f in range(0, len(model_final_features)):\n",
    "    feat = 'f' + str(f)\n",
    "    out_feature_list.append(feat)\n",
    "\n",
    "feat_df = pd.DataFrame(data = model_final_features, index = out_feature_list, columns= ['feature_names'])\n",
    "feat_df['feature_names'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "st.set_page_config(layout=\"wide\")\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "\n",
    "# test = xgb.DMatrix(X_test_trf[i].reshape(-1,1))\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(training_data = X_train_trf,\n",
    "                                                feature_names = feat_df['feature_names'].to_list(), \n",
    "                                                class_names=['1','0'],\n",
    "                                                verbose = True,\n",
    "                                                mode = 'classification',\n",
    "                                                kernel_width=3\n",
    "                                                )\n",
    "\n",
    "predict_fn_xgboost = lambda x: xgboost_model.predict_proba(x).astype(float)\n",
    "exp = explainer.explain_instance(X_test_trf[i],predict_fn_xgboost, num_features= 10)\n",
    "exp.show_in_notebook(show_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final_features = model_input_pipe.get_feature_names_out(input_features= FEATURES)\n",
    "\n",
    "out_feature_list = []\n",
    "for f in range(0, len(model_final_features)):\n",
    "    feat = 'f' + str(f)\n",
    "    out_feature_list.append(feat)\n",
    "\n",
    "feat_df = pd.DataFrame(data = model_final_features, index = out_feature_list, columns= ['feature_names'])\n",
    "# feat_df\n",
    "temp_fea_df = pd.DataFrame(X_train_trf, columns= feat_df['feature_names'].to_list())\n",
    "temp_fea_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(temp_fea_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(xgboost_model)\n",
    "shap_values = explainer.shap_values(temp_fea_df)\n",
    "expected_value = explainer.expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_img = shap.plots._waterfall.waterfall_legacy(expected_value, shap_values[79], features = temp_fea_df.loc[79,:], feature_names=temp_fea_df.columns, max_display=15, show=False)\n",
    "plt.tight_layout()\n",
    "explainer_img.savefig('shap_feature_importance.jpg', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"xgb_explainer.b\", \"wb\") as f_out:\n",
    "            pickle.dump(explainer, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(shap_values[79])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_fea_df.loc[79,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Persistency-fo3amxwL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "8b1331eaecbea1bb1121a8441a5893a9fe826dcb8cbb4f9ece883bb1402df7ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
